%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required

\RequirePackage{fix-cm}

%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn

\smartqed  % flush right qed marks, e.g. at end of proof


% \usepackage{mathptmx}      % use Times fonts if available on your TeX system

\usepackage{float}
\usepackage{natbib}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{color}

\definecolor{commentcolour}{rgb}{0.4,0.4,0.4}
\definecolor{keywordcolour}{rgb}{0.8,0.4,0.4}
\definecolor{stringcolour}{rgb}{0.1,0.1,0.5}

\lstset{
    captionpos=b,                    % sets the caption-position to bottom
    language=Python,                 % the language of the code
    showstringspaces=false,          % underline spaces within strings only
    stringstyle=\color{stringcolour},     % string literal style
    commentstyle=\color{commentcolour},    % comment style
    keywordstyle=\color{keywordcolour}\bfseries,       % keyword style
    float
}

\newfloat{lstfloat}{htbp}{lop}
\floatname{lstfloat}{Listing}
\def\lstfloatautorefname{Listing}

\newcommand{\D}{...}
\newcommand*{\affaddr}[1]{#1}
\newcommand*{\affmark}[1][*]{\textsuperscript{#1}}


% Insert the name of "your journal" with
\journalname{Neuroinformatics}

\begin{document}

\title{A comprehensive framework to capture the arcana of neuroimaging
analysis
}

%\titlerunning{Short form of title}        % if too long for running head

\author{
	Thomas G. Close\affmark[1,2]*\and
	Phillip G. D. Ward\affmark[1,3,4] \and
	Francesco Sforazzini\affmark[1] \and
	Wojtek Goscinski\affmark[6] \and
	Zhaolin Chen\affmark[1,5] \and
	Gary F. Egan\affmark[1,3,4]
}

\authorrunning{
     Thomas G. Close \and
	Phillip G. D. Ward  \and
	Francesco Sforazzini  \and
	Wojtek Goscinski  \and
	Zhaolin Chen  \and
	Gary F. Egan}

\institute{
	Thomas G. Close \at
	 \email{tom.close@monash.edu} \\ \\
         \affaddr{\affmark[1]Monash Biomedical Imaging, Monash University, Melbourne, Australia} \\
         \affaddr{\affmark[2]Australian National Imaging Facility, Australia} \\
         \affaddr{\affmark[3]Australian Research Council Centre of Excellence for integrative Brain Function, Melbourne, Australia} \\
         \affaddr{\affmark[4]Monash Institute of Cognitive and Clinical Neurosciences, Monash University, Melbourne, Australia} \\
         \affaddr{\affmark[5]Department of Electrical and Computer Systems Engineering, Monash University, Melbourne, Australia} \\
         \affaddr{\affmark[6]Monash eResearch Centre, Monash University, Melbourne, Australia} \\
}

%\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

\begin{abstract}

Mastering the ``arcana of neuroimaging analysis'', the obscure knowledge required to apply an
appropriate combination of software tools and parameters to analyse a given neuroimaging dataset,
is a time consuming process. Therefore, it is not typically feasible to invest the additional effort
required generalise workflow implementations to accommodate for the various acquisition parameters,
data storage conventions and computing environments in use at different research sites,
limiting the reusability of published workflows.

We present a novel software framework, \emph{Abstraction of Repository-Centric ANAlysis (Arcana)},
which enables the development of complex, ``end-to-end'' workflows that are adaptable to new analyses and
portable to a wide range of computing infrastructures. Analysis templates for specific image types (e.g. MRI contrast)
are implemented as Python classes, which define a range of potential derivatives and analysis methods.
Arcana retrieves data from imaging repositories, which can be BIDS datasets, XNAT instances or plain directories, and
stores selected derivatives and associated provenance back into a repository for reuse by subsequent analyses.
Workflows are constructed using Nipype and can be executed on local workstations or in high performance
computing environments. Generic analysis methods can be consolidated within common
base classes to facilitate code-reuse and collaborative development, which can be specialised for study-specific
requirements via class inheritance. Arcana provides a framework in which to develop unified neuroimaging
workflows that can be reused across a wide range of research studies and sites.

\keywords{Neuroimaging \and Workflows \and Repository \and Reproducibility \and Reusability \and Large-scale \and Python}

\end{abstract}

\section*{Introduction}
\label{intro}

Despite the availability of well-established neuroimaging analysis
packages \citep{cox_afni:_1996,smith_advances_2004,friston_statistical_2007,tournier_mrtrix:_2012},
the arcana of neuroimaging analysis is substantial due to
the range of available tools, tuneable parameters, and imaging sequences
involved \citep{cusack_automatic_2015}. The distribution of complete
``end-to-end'' workflows, from acquired data to publication results, is
necessary for routine reproduction because of the effort required to
accurately reimplement such analyses \citep{kennedy_neuroimaging_2018}. It is also
difficult to adapt existing workflows to new studies without detailed
knowledge of their design. Therefore, flexible, portable and complete
workflows are important to promote reproduction and code reuse in
neuroimaging research.

A barrier to designing portable and complete workflows is the
heterogeneity of data storage conventions \citep{marcus_extensible_2007,das_loris:_2012,gorgolewski_brain_2016}.
To address this, the emerging Brain
Imaging Data Standard (BIDS) \citep{gorgolewski_brain_2016} provides a way
to standardise the storage of neuroimaging data in file system
directories. BIDS specifies strict file and directory naming
conventions, which facilitate the design of portable \emph{BIDS Apps}
(\url{bids-apps.neuroimaging.io}). However, for research groups with
sufficient informatics support, software-managed repositories \citep{marcus_extensible_2007,das_loris:_2012}
can provide additional features, such as
flexible access-control and automated pipelines. For
published workflows, the choice of repository should be transparent in
order to maximise their audience.

While neuroimaging analyses are generally amenable to standardisation
\citep{kennedy_neuroimaging_2018}, minor modifications are often required to accommodate
idiosyncrasies of the acquisition protocols in use at different sites
\citep{esteban_fmriprep:_2018}. Workflows may require conditional
logic in construction or execution to be portable. Nipype is a flexible
Python framework for neuroimaging analysis in which workflows are
constructed programmatically in Python \citep{gorgolewski_nipype:_2011}.
Programmatic construction allows for rich control-flow logic that is not
readily available in alternative workflow frameworks \citep{cusack_automatic_2015,achterberg_fastr:_2016,amstutz_common_2016},
and has been used
to implement workflows that are robust to differences in fMRI protocols
across a large number of sites \citep{esteban_fmriprep:_2018}.

The trend towards large multi-site and multi-contrast datasets collected
over a number of years \citep{van_essen_human_2012,thompson_enigma_2014,sudlow_uk_2015}
presents additional challenges to workflow design.
Analysis packages are constantly being developed and improved, so the
state-of-the-art workflow for a particular analysis can change over
time. Therefore, it is challenging to ensure workflows are applied
consistently over the course of long studies \citep{cusack_automatic_2015}.

While analysis workflows for different contrasts and modalities are
typically implemented independently, they can share common processing
steps (e.g. non-linear registration to standard space, surface
parcellation) and their outputs may need to be integrated to produce
publication results. For large scale studies, which are typically
processed on the cloud or high-performance computing (HPC) clusters,
rerunning common segments can lead to significant increases in
computation time and project cost. In addition, duplication of
processing segments increases time for manual quality control (QC),
making the reuse of intermediate derivatives a practical requirement for
some large studies \citep{schreiber_using_2018}.

To maximise the reusability of neuroimaging workflows and avoid frequent
reimplementation of standard analyses, workflow implementations should
be flexible, extensible and applicable to a wide range of storage
systems. In addition, in order to promote routine reproduction of
neuroimaging studies, published workflow implementations should include
the complete procedure, from acquired data to publication results.
However, ensuring workflow implementations are flexible, portable and
complete adds a high degree of complexity and effort to the design
process.

Our objective was to extract common elements of repository-centric
workflow design into an abstract framework to make it practical to
implement flexible, portable and complete workflows for a wide range of
neuroimaging analyses. \emph{Abstraction of Repository-Centric ANAlysis
(Arcana)} (\url{arcana.readthedocs.io}) is a Python framework for
designing complex workflows in which modular Nipype pipelines operate on
data stored in repositories. Intermediate derivatives are
derived on demand, checking against stored provenance for required
updates. Analyses can be applied to XNAT, BIDS and plain-directory
repositories, and using Nipype's execution plugins, run on workstations
or be submitted as batch jobs to HPC schedulers. Arcana's architecture,
with programmatic workflow construction---yet clear delineation between
analysis design and application---facilitates the implementation of
complex workflows that are portable and complete.

The utility of the Arcana framework is demonstrated by the
implementation of analysis suites for T1, T2* and diffusion weighted MRI
(DWI) data and the application of DWI tractogram \citep{tournier_mrtrix:_2012} and
vein masks \citep{ward_vein_2017} workflows to data collected from a
healthy subject.

\section*{Methods}
\label{methods}

\subsection*{Framework overview}
\label{framework-overview}

The separation of analysis design and application in the Arcana
framework follows the conceptual divide between classes and objects in
Object-Oriented (OO) software design. \emph{Study} classes encapsulate
types of data, such as scans of a specific imaging
contrast or modality, with the suite of analysis methods that can be performed on
them. Study objects apply the analysis suite defined in
the Study class to a specific dataset.

The set of input data, the derivatives that can be derived from it,
and methods that construct pipelines to derive the derivatives, are
linked together by the \emph{data specification} class attribute of the
Study (Figure \ref{fig:example_study}). Likewise,
free parameters used in pipeline construction are defined in the class'
\emph{parameter specification} class attribute. Class inheritance can
be used to specialise analysis suites by overriding entries in the
specifications or pipeline constructor methods. Analysis suites
for multi-modal data can be implemented by combining Study classes
within \emph{MultiStudy} classes\emph{.}

\begin{figure*}
    \centering
    \includegraphics[width=0.55\textwidth]{../figures/example_study}
  \caption{Example study. Blue boxes represent input
data (filesets or fields) stored in a repository and green derivatives
from that data stored alongside the original data. Orange ovals are
pipelines that operate on data in the repository to derive the
derivatives. Arrows represent data flows, i.e., inputs and outputs to
pipelines}
\label{fig:example_study}
\end{figure*}

Analysis methods defined by a Study class are applied to a
specific dataset by instantiating an object of the class and
requesting a derivative listed in the class' data specification.
At initialisation, a Study object is passed references to a \emph{Repository},
a \emph{Processor}, and an \emph{Environment} objects, which
define where and how data is stored and processed. When a derivative is
requested, a Study object queries the Repository for intermediate
derivatives that can be reused before constructing a workflow to produce
the requested derivative. The manner in which the workflows are executed
(i.e. single/multi-process or via a SLURM scheduler \citep{yoo_slurm:_2003})
is specified by the Processor and software modules required by the analysis are
loaded by the Environment. Selected workflow products are
stored back in the repository for reuse by subsequent analyses (Figure
\ref{fig:simple_uml}). Input data to a study are selected from repositories
 using criteria defined in \emph{Input} objects passed to the Study
 object at initialisation and matched against entries in the class' data
specification.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/simplified_arcana_uml}
  \caption{Unified Modelling Language (UML) diagram
of information flow in the Arcana framework. Boxes: Python classes,
blue=analysis-design, green=analysis-application. Arrows: orange=data,
magenta=workflow description, diamond=aggregated-in. Study classes
construct analysis pipelines, which are sent to the \emph{Processor} to
be processed. Input data is selected by \emph{Input} objects and
pulled to the compute environment to be processed along with existing
intermediate derivatives. After the derivatives are pushed back to the
repository.}
\label{fig:simple_uml}
\end{figure*}

\subsection*{Analysis design: Study classes}
\label{analysis-design-study-classes}

Study classes encapsulate a study dataset (e.g. data collected across
multiple subjects using the same acquisition protocol) with the suite of
analytical methods that can be applied to the dataset. The hierarchy of
a study dataset is assumed to have two levels, \emph{subjects} and
\emph{sessions}, with each session for each subject corresponding to a
specific \emph{visit}, e.g. timepoint in longitudinal study. Derivatives
can be created at any point in this hierarchy: per-session, per-subject,
per-visit and per-study. Iteration over subjects and visits is handled
implicitly by the framework. All Study classes must inherit from the
\emph{Study} base class and be created by the \emph{StudyMetaClass}
metaclass or subclasses thereof.

\subsubsection*{Data and parameter specifications}
\label{sec:data-and-parameter-specifications}

At the heart of each Study class is the data specification class attribute, which
specifies the input and output data of the analysis, and all
stages in between. There is a one-to-one relationship between entries in
the data specification and derivatives that are stored in the repository,
or will be stored if and when they are derived. Which intermediate
derivatives to include in the data specification, and therefore
store in the repository, is left to the discretion of the researcher
designing the analysis. However, as a general rule, derivatives that
require manual QC or are likely to be reused between different branches
of analysis should be included in the data specification.

Entries in the data specification can be of either a
\emph{file-set} or \emph{field} type. File-sets represent single files,
or sets of related files typically contained within a directory
(e.g. a multi-volume DICOM dataset). Fields
represent integer, floating point, date-time, or unicode string
values. By default, a field contains a single value but if the \emph{array}
flag is set, a field can contain a list of values.
Each file-set references a \emph{FileFormat} object, which
specifies the formats of the files in the set. File formats are
explicitly registered by the researcher at design time using the
\emph{FileFormat.register(format)} class method to avoid conflicts where
the same extension is used for different formats in different contexts.

The data specification of a Study class are defined by providing a
list of \emph{FilesetSpec} and \emph{FieldSpec} objects in the
\emph{add\_data\_specs} class attribute (Figure \ref{fig:data_spec}).
The lists are collated with data specifications in any base Study
classes during the creation of the class. Input data to the study are
distinguished from derivatives by using the \emph{FilesetInputSpec}
and \emph{FieldInputSpec} subtypes. However, the distinction between
input and derived data is somewhat blurred, as derivative specification
entries can be passed input data when the class is instantiated. In addition,
when subclassing a Study class or aggregating it in a MultiStudy class,
derivative entries can be overridden by input entries, and vice-versa.

\begin{figure}
\lstinputlisting[basicstyle=\footnotesize,firstline=2, lastline=59]{../code/example_study.py}
\caption{Example data and parameter specifications. The data
specification specifies two input file-sets, `one' and `ten' and
ten derived file-sets that can be derived from them, at least
indirectly. Each derived data spec, specifies the name of the pipeline
constructor that creates the pipeline that derives them. Parameter
specifications specify a name and default value for free parameters of
the Study class}
\label{fig:data_spec}
\end{figure}

All data specification entries have a \emph{frequency} attribute which specifies
where the data sits in the hierarchy of the
dataset and can take the values 'per\_session',
'per\_subject, 'per\_visit' or 'per\_study'. In
addition, derived specification entries are passed the name of a method in the
class that constructs the pipeline to derive them. Therefore, while a
pipeline can have multiple outputs, each derivative is derived by only
one pipeline.

For Study classes that correspond to a known type in the BIDS
standard, a dictionary mapping their data specifications to default
\emph{BidsInput} or \emph{BidsAssocInput} selectors can be specified in the
\emph{default\_bids\_inputs} class attribute. BidsInput
specifies a primary scan in the BIDS standard using its \emph{type}, \emph{modality},
\emph{format} and optionally the \emph{task} it belongs to. BidsAssocInput is used to select
associated files, such as field maps and diffusion encoding matrices relative
to a primary scan. Default BIDS selectors are typically not provided with
a value for the \emph{task} keyword since the task can be set at instantiation of
the Study class with the \emph{bids\_task} keyword argument, which enables the design of Study
classes that are not restricted to a single task by default.

Similar to data specifications, parameter specifications are included in
the Study class by providing a list of \emph{ParamSpec} objects to
the \emph{add\_param\_specs} class attribute. ParamSpec objects
are initialised with a name and default value. Special parameters that
specify a qualitative change in the analysis, for example using ANTs
registration \citep{avants_reproducible_2011} instead of FSL registration
\citep{smith_advances_2004}, are specified by the \emph{SwitchSpec} subtype.
SwitchSpecs take a name, default value and a list of accepted values at
initialisation.

\subsubsection*{Workflow design: pipeline constructor methods}
\label{sec:pipeline-constructors}

Workflows are implemented in Arcana as a series of modular pipelines, which
each perform a unit of the analysis (e.g. registration, brain
extraction, quantitative susceptibility mapping). Pipelines are represented by \emph{Pipeline} objects,
which are thin wrappers around Nipype workflows to handle input and output
connections, and namespace management. Each pipeline consists of a
(typically small) graph of Nipype nodes, with each node wrapping a stand alone tool
(e.g. FSL's FLIRT) or analysis package function (e.g. SPM's \emph{coreg}
tool). Pipelines source their inputs from, and sink their outputs to, entries
in the data specification, thereby linking the specification together.

Pipelines are constructed dynamically by ``pipeline constructor" methods
of the study. Pipeline constructor methods are referenced by name in the
data specifications they derive. Pipeline constructor methods should only
receive wildcard keyword arguments (e.g. \lstinline{my_pipeline(self, **name_maps)}),
and these arguments should be passed as a dictionary directly to the \emph{name\_maps} argument
of the pipeline initialisation to enable inputs and outputs of the
pipeline to be rerouted to alternative data specifications in modified constructor
methods, typically in subclasses and multi-studies (see
\emph{Extension and specialisation by class inheritance} and \emph{Implementing multi-modal studies}).

\begin{figure}
\lstinputlisting[firstline=61, basicstyle=\footnotesize]{../code/example_study.py}
\caption{Example pipeline constructor method. Pipelines are
created using the \emph{pipeline} method of the Study class.
Pipeline objects are thin wrappers around Nipype Workflow objects to
in order manage the namespaces of the workflow's inputs, outputs and nodes. Every
pipeline constructor method should allow wildcard keyword arguments,
which are passed to the \emph{name\_maps} argument of the pipeline
initialisation. This allows pipeline constructors in sub and multi classes to map the
inputs and outputs of the pipeline onto different data specifications.}
\label{fig:pipeline_constructor}
\end{figure}

The syntax for pipeline construction is inspired by changes proposed for Nipype v2.0
(\url{github.com/nipy/nipype/issues/2539}).
Within a pipeline constructor method, a Pipeline object is constructed by the \emph{Study.new\_pipeline} method
(Figure \ref{fig:pipeline_constructor}). At initialisation, each pipeline is assigned a name,
which must be unique amongst the pipelines constructed by the Study given any combination of
switches. The methods implemented by the pipeline can be characterised by providing a the list of citations
and a text description to the \emph{citations} and \emph{desc} keyword arguments, respectively.

The \emph{add(name, interface)} method is used to add a node to a pipeline. It takes
a unique name for the node (within the pipeline) and a Nipype Interface object, and returns
a reference to the newly added node. For clarity, it is recommended to put all static inputs
(i.e. parameters) of the interface as keyword arguments of the interface constructor
(Figure \ref{fig:pipeline_constructor}). However, if an input conditionally depends on a parameter of the
study it can be set via the \emph{inputs} attribute of the returned Nipype node
(e.g. \lstinline{my_node.inputs.my_param = 1.0}).

Node inputs and ouputs are connected to each other, and to inputs and outputs of the pipeline, by
providing \emph{inputs} and \emph{outputs} keyword arguments to the \emph{add} method. Both
arguments take a dictionary. The keys of the \emph{inputs} dictionary correspond to
trait names in the node's input specification, whereas the keys of the \emph{outputs} dictionary correspond to
names of entries in the study data specification. The values of both dictionaries are 2-tuples. For pipeline
inputs, values of the \emph{inputs} dictionary consist of a name of an entry in the study data specification
and the format the input data is expected in (i.e. a FileFormat for Fileset specifications or core Python type for Field specifications).
For pipeline outputs, values of the \emph{outputs} dictionary consist of a trait name in the node's output specification and the format the
output data is generated in. For input connections from other nodes, values of the \emph{inputs} dictionary
consist of a reference to the upstream node and the name of a trait in the
upstream node's output specification (output connections to other nodes are implied by input connections to the receiving nodes).

If the expected format of a pipeline input or generated format of a pipeline output does not match that
of the corresponding study input or data specification, then a conversion node is implicitly connected
to the pipeline by the framework to perform the required conversion. Pipeline inputs and outputs
that are conditional on parameters or inputs provided to the study can be specified outside of the
\emph{add} method using the \emph{connect\_input} or
\emph{connect\_output} methods, respectively. Conditional connections between
nodes can be specified via the \emph{connect} method of the pipeline (which simply calls the
method of the same name in the underlying Nipype workflow).

Any external software packages required by a node should be referenced
in the \emph{requirements} keyword argument as a list of
\emph{Requirement} objects when the node added to the pipeline.
Similarly, the expected memory requirements in MB and
wall time for the node execution should be provided to the keyword arguments
\emph{mem\_gb}, and \emph{wall\_time}.

Iteration over subjects and visits is handled implicitly by Arcana and
depends on the frequency of the pipeline's inputs and outputs. To create
a summary derivative (i.e. frequency != 'per\_session') from more frequent
data, \emph{Study.SUBJECT\_ID} or \emph{Study.VISIT\_ID} should be
passed to the \emph{joinsource} keyword of the \emph{add} method to join over subjects or visits, respectively.
In this case, a JoinNode will be created instead of standard Node,
which should be passed the additional keyword argument \emph{joinfield} to
specify the list of input traits to convert into lists to receive the joined input.
Similarly, if the name of an input trait is provided (or list thereof) to the the \emph{iterfield}
keyword argument, then a MapNode will be created and the interface will
be applied to all items of the list connected to that input \citep{gorgolewski_nipype:_2011}.
Additionally, the values of the subject and visit IDs are directly accessible as input fields of the pipeline
named \emph{Study.SUBJECT\_ID} and \emph{Study.VISIT\_ID}, respectively.

Study parameters can be accessed during pipeline construction with the
\emph{Study.parameter(name)} method. If conditional logic is included in
the workflow construction that alters the pipeline inputs, outputs or
parameters then it should be controlled by a switch instead of a
parameter. The analysis branch designated by a switch value should be
tested with \emph{Study.branch(switch\_name)} in the case of boolean switches and
\emph{Study.branch(switch\_name, branch\_name)} in the case of string switches.

\subsubsection*{Extension and specialisation by class inheritance}
\label{extension-and-specialisation-by-class-inheritance}

Because Arcana analyses are implemented as Python classes, class
inheritance can be used to specialise existing analyses.

Instead of being set directly, the data and parameter specifications are set
by the metaclass of the Study (i.e. StudyMetaClass)
in order to combine them with corresponding specifications in base classes.
The combined data and parameter specifications are constructed by visiting the class'
bases in reverse method resolution order (MRO) and adding specifications
from their \emph{add\_data\_specs}, \emph{add\_param\_specs} attributes,
overriding previously added specifications with matching names. Note
that in this scheme, specifications can only be appended or overridden
but not removed by Study subclasses so as not to break workflows
inherited from base classes.

Pipeline constructor methods can be overridden in subclasses like any Python
method. Often the overriding method will call the superclass method
to construct a pipeline, apply modifications and return the
modified pipeline. In this scenario, references to the data
specification in the superclass method can be mapped onto different
entries in the specification of the subclass by providing the \emph{input\_map} and
\emph{output\_map} keyword arguments when calling the superclass method.
Additionally the name of the pipeline can be altered by providing the \emph{name}
argument, which is useful when creating multiple pipeline constructor methods
from a single base method. In these scenarios it is important to
also pass the the wildcard keyword arguments of the overriding method to the
\emph{name\_maps} keyword argument of the superclass method to allow the
overriding method to be overridden in turn.


\subsubsection*{Implementing multi-modal studies: MultiStudy classes}
\label{implementing-multi-modal-studies}

While basic Study classes are typically associated with single image
modality or contrast, the analysis suites implemented by them can be
integrated into multi-modal analysis by aggregating multiple Study
classes (sub-studies) in a \emph{MultiStudy} class. Analysis suites are
integrated in a MultiStudy class by linking the data specifications
of sub-studies. Sub-study specification entries are linked by mapping
them to a common entry in the specification of the MultiStudy class
(Figure \ref{fig:example_multi_study}). This enables derivatives from one
sub-study (e.g. brain extracted T1-weighted anatomical) to be referenced
by workflows of other sub-studies (e.g. anatomically constrained DWI
tractography).


\begin{figure*}
  \centering
    \includegraphics[width=0.775\textwidth]{../figures/example_multi_study}
  \caption{Example MultiStudy. Blue boxes represent input data
(filesets or fields) and green derivatives. Orange ovals
are pipelines. Blue and green arrows: pipeline inputs from study inputs
and derived data, respectively. Orange arrows: outputs of pipelines. Dashed
boxes represent data specifications in a sub-study that are present in
the global namespace and mapped into the sub-study space, and dotted
arrows the mappings. Sub-studies are linked by mapping the same data
spec in the global space onto data specifications the multiple sub-study
namespaces (e.g. \emph{Derived} \emph{1}, \emph{2} and \emph{3}).
There are no restrictions between mapping study input and derivative data
specifications: both input and derivative specifications can be mapped
onto input data or derivative specifications in sub-studies.
If a spec in the global spaced is mapped onto a derivative spec in the
sub-study space, then the pipeline that generates that derivative in the
sub-study will not run unless it generates other required derivatives
(e.g. \emph{Pipeline 3} in \emph{Sub-Study 2})}
\label{fig:example_multi_study}
\end{figure*}

All MultiStudy classes must inherit from the Multi\emph{Study} base
class and be created by the \emph{MultiStudyMetaClass} metaclass or
subclasses thereof. As in the case of subclassing the standard Study
class, additional data and parameter specifications can be added to the
class via \emph{add\_data\_specs} and \emph{add\_param\_specs}
respectively for additional analysis not included in the sub-studies.

Sub-studies are aggregated in the \emph{sub-study} \emph{specification}
of a MultiStudy class via a list of \emph{SubStudySpec} objects
in the \emph{add\_substudy\_specs} class attribute in the manner of
data and parameter specifications. A SubStudySpec consists of a name, a
Study class, and a \emph{name map} dictionary\emph{.} The name map
dictionary maps data and parameter specification names from the
sub-study namespace to the namespace of the MultiStudy class, i.e.
the dictionary keys refer to entries in the sub-study specification and
the dictionary values refer to entries in the multi-study specification.

Entries in the specifications of sub-study classes that are not
referenced in the sub study's name map are implicitly mapped to the
MultiStudy namespace by the MultiStudyMetaClass during construction of the
MultiStudy class using the name of the sub-study as a prefix.
If the implicitly mapped specification is derived, then its associated pipeline
constructor method is also mapped into the MultiStudy namespace with a prefix.
For example, \emph{derived1} in \emph{substudy2} would be mapped to
\emph{substudy2\_derived1} along with the method
\emph{substudy2\_pipeline1}.

\subsection*{Analysis application: Study instances}
\label{analysis-application-study-instances}

To apply the analysis blueprint specified in a Study class to a specific
dataset, an instance of the Study class is created with details of where
the data is stored (\emph{Repository} module and \emph{Input} selectors) the computing
resources available to process it (\emph{Processor} module) and the software
installed in the environment it will be processed in (\emph{Environment} module).
A Study object controls the construction and execution of analysis workflows,
and the flow of data to and from the repository (Figure \ref{fig:full_uml}).

\begin{figure*}
	\centering
    \includegraphics[width=\textwidth]{../figures/full_arcana_uml}
  \caption{Detailed Unified Modelling Language (UML) diagram of
information flow in the Arcana framework. Boxes: Python classes
(blue=core, green=interchangeable modules, grey=example
specialisations). Arrows: orange=data, magenta=workflow description,
diamond=aggregated-in, triangle=subclass-of. Calling \emph{data(name)}
on a Study subclass constructs the requisite pipelines (as specified in
\emph{data\_specs}) to produce the requested data, and sends them
to the \emph{Processor} to be processed. Data is selected by
\emph{Input} objects, pulled to the compute environment to be
processed, and then the derivatives are pushed back to the repository.
Repositories can be of plain directories, or BIDS or XNAT repositories}
\label{fig:full_uml}
\end{figure*}

Each Study instance is assigned a name, which is used to differentiate its
results from alternative analyses on the same dataset (e.g. with different
parameterisations). Parameters are set on initialisation of the Study object
along with the range of subject and visit IDs to be included in the analysis.
If subject and visit IDs are not explicitly provided, then all IDs found in the repository are included.
The remaining arguments passed to the Study object at initialisation are the
Repository, Processor and Environment modules and a list of
\emph{Input} objects to match data in the repository with inputs in the
data specification (Figure \ref{fig:study_application}).

\begin{figure}
\lstinputlisting[basicstyle=\footnotesize]{../code/example_application.py}
\caption{Example application of Study class to a dataset stored in an XNAT
repository. Once the Study object has been initialised potential
derivatives of the Study can be requested, and will be generated and
stored in the repository if already present.}
\label{fig:study_application}
\end{figure}

\subsubsection*{Repository modules}\label{repository-modules}

In Arcana, repository access is encapsulated within modular
\emph{Repository} objects to enable switching between different
repositories and repository types at analysis ``application time''
(Figure \ref{fig:simple_uml}). There are currently three supported repository types, XNAT
\citep{marcus_extensible_2007}, BIDS \citep{gorgolewski_brain_2016} and a
``basic'' format, which are encapsulated by \emph{XnatRepo},
\emph{BidsRepo}, and \emph{BasicRepo} classes
respectively.

In its most basic form, a ``basic'' repository, is just a file system
directory containing the data to be processed for a single subject.
For multi-subject studies, the root directory should contain separate
subdirectories for each subject in the study, with subject IDs taken
from the subdirectory names. If data was acquired for each subject
over multiple visits, than an additional layer of nested subdirectories
is included in the subject subdirectories, with the visit IDs taken from
the subdirectory names. The basic
repository is similar to the BIDS format in this form, however, there are no naming
conventions in the basic repository, which enables prototyping and
testing of analyses on loosely structured data.

Derivatives are stored by their specification name in a study-specific
namespaces to avoid clashes between separate analyses. In basic
repositories this namespace is a subdirectory named after the study
nested within the lowest layer of the data tree. In BIDS repositories, the namespace is a subdirectory of the
\emph{derivatives} directory, again named after the study. In XNAT
repositories, derivatives for each session are stored in separate
\emph{MrSession} objects alongside the primary session underneath its
\emph{Subject}, and are named
\emph{\textless{}primary-session-name\textgreater{}\_\textless{}study-name\textgreater{}}
(Table \ref{tbl:repo-structure}).

Derived filesets are stored with the format specified in the study's
data specification. In basic and BIDS repositories, fields are
stored in a single JSON file named `\_\_fields\_\_.json' in each derived
session, and on XNAT they are stored in custom variables in the derived
session. Provenance is stored in a `\_\_prov\_\_' sub-directory (dataset
on XNAT) of the derivatives directory (MrSession on XNAT) in separate
JSON files for each pipeline (Table 1).

Summary data (i.e. with \emph{per\_subject}, \emph{per\_visit}, and
\emph{per\_study} frequencies\emph{)} are stored in specially named
subjects and visits (e.g. `group'), the names for which are specified when
the repository is initialised. For example, given a basic
repository using `group' as the summary name for both subjects and visits,
per\_subject data for `subj1' would be stored at
\emph{\textless{}root\textgreater{}/subj1/}group, per\_visit data for
`visit1' in \emph{\textless{}root\textgreater{}/}group\emph{/visit1},
and per\_study data in \\ \emph{\textless{}root\textgreater{}/}group/group
(Table \ref{tbl:repo-structure}).

\begin{table}
\caption{Storage locations of derived data for each repository
type. Derivatives are stored in separate namespaces for each Study
instance to enable multiple analyses on the same datasets with different
parameterisations. Where `...' is the location of the
directory or MrSession that holds the derivatives, \emph{subj} = subject ID, \emph{vis} = visit ID,
\emph{study} = name of the Study instance, and \emph{pl} = name
of pipeline.}
\label{tbl:repo-structure} 
\begin{tabular}{llll}
\hline\noalign{\smallskip}
\textbf{Datatype} & \textbf{Plain-directory} & \textbf{BIDS} & \textbf{XNAT}  \\
\noalign{\smallskip}\hline\noalign{\smallskip}
Derivatives & /\emph{subj}/\emph{vis}/\emph{study} & /derivatives/\emph{study}/\emph{subj}/\emph{vis} & /\emph{subj}/\emph{vis}\_\emph{study}\\
Fields & \D/\_\_fields\_\_.json & \D/\_\_fields\_\_.json & MrSession XML \\ 
Provenance & \D/\_\_prov\_\_/\emph{pl}.json & \D/\_\_prov\_\_/\emph{pl}.json & \D/\_\_prov\_\_/\emph{pl}.json \\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

Each study can only have one repository in which derivatives are stored.
However, a study can draw data from multiple auxiliary repositories,
which are specified in the inputs passed to the study. When using
multiple input repositories, subject and visit IDs will often need to be
mapped from their values in the auxiliary repositories to the ``ID
space'' of the study, which can be done by passing either by a dictionary
or callable object (e.g. function) to the \emph{subject\_id\_map} or
\emph{visit\_id\_map} keyword arguments during initialisation of a repository.

New repository modules for additional repository types can be
implemented by extending the Repository abstract base class and
implementing five abstract methods, \emph{find\_data}, \emph{get\_fileset},
\emph{get\_field}, \emph{put\_fileset} and \emph{put\_field}
(Table \ref{tbl:abstract-repo-methods}).

\begin{table}
\caption{Abstract methods in the base Repository class that
need to be implemented by platform-specific sub-classes.}
\label{tbl:abstract-repo-methods} 
\begin{tabular}{ll}
\hline\noalign{\smallskip}
\textbf{Method name} & \textbf{Function} \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\emph{find\_data()} & Queries repository to find all existing filesets and
fields \\
\emph{get\_fileset(Fileset)} & Caches fileset (if necessary) and returns the path to file(s). \\
\emph{get\_field(Field)} & Retrieves and returns the value of the field from
the repository \\
\emph{put\_fileset(Fileset)} & Inserts fileset into the repository and updates cache. \\
\emph{put\_field(Field)} & Inserts the value of the Field into the repository \\
\emph{connect()} & Opens a connection to the repository (optional). \\
\emph{disconnect()} & Closes connection to repository (optional)\\
\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\subsubsection*{Study inputs}
\label{study-inputs}

While derivatives generated by a Study object are named in accordance
with the data specification of the Study class, input file-sets and fields
do not need specific names when using basic or XNAT
repositories. In these cases, a selection stage matches input data to
entries in the data specification of the Study class. The criteria for
these selections are passed to the
the \emph{inputs} argument of the Study object at instantiation in a
dictionary mapping data specification names to \emph{FilesetInput} and
\emph{FieldInput} objects.

FilesetInput and FieldInput objects are required
to match exactly one fileset or field in every session included in the study.
Matching is typically performed on file
names (dataset labels for XNAT repositories) and field names. If the
names are inconsistent across the study then regular expressions can be
used instead of exact matches with the \emph{pattern} keyword argument.
Additional criteria can be used to distinguish cases where multiple
filesets in the session match the pattern provided, such as DICOM
header values, or the order and ID of the dataset.

Inputs can be drawn from auxiliary repositories by providing alternative
Repository instances to the \emph{repository} keyword of the Input.
Care should be taken to ensure that the subject and visit ID schemes
will map correctly to that of the primary repository (see
\emph{Repository modules}). If the primary repository is empty (i.e. all
inputs come from auxiliary repositories) then explicit
\emph{subject\_ids}, \emph{visit\_ids} need to be provided and the
\emph{fill\_tree} flag set when initialising the Study.

If the data to select has been derived from an alternative Study
instance, then the name of the alternative study can be passed to the
\emph{from\_study} keyword argument provided to the Input object. There are
no restrictions on selecting any data, derived or otherwise, to match
a specification entry. For example, it is possible to
deliberately skip analysis steps by selecting an upstream derivative
from a workflow that was run previously as a match for a downstream derivative,
although this is not recommended as standard practice.

Specific file-sets and fields can be passed as inputs to a Study in
\emph{FilesetCollection} and \emph{FieldCollection}
objects. Collection objects reference an entry in the data specification
and contain a single Fileset or Field for every session (or
subject/visit/study depending on the frequency of the corresponding data
specifcation). Collection objects can be used to pass reference atlases and
templates as inputs to analyses. They can also be set as the default
for a data specification entry via the \emph{default} keyword
argument. However, for the sake of portability, default inputs should be
restricted to data in publically accessible repositories or those included in
standard software packages (e.g. FSL).

When using BIDS repositories, the input selection stage is typically
included in the data specification (see \emph{Data and parameter
specifications)} so inputs do not need to be provided to the
initialisation of the Study. However, BidsInput and
BidsAssocInput objects can be provided as inputs to override the default
selections if required.

\subsubsection*{Processor modules}
\label{processor-modules}

Processor modules control how pipelines generated by a Study
are executed. There currently three Processor modules implemented in
Arcana: \emph{SingleProc}, \emph{MultiProc},
\emph{SlurmProc}, which wrap the LinearPlugin, MultiProcPlugin and SlurmPlugin
Nipype execution plugins, respectively. The main task performed
by the processor, as separate from the Nipype execution plugin it wraps,
is to determine which pipelines need to be run and link them into a
single workflow. Since this logic is implemented in the Processor
abstract base class, wrapping additional Nipype plugins as required is
straightforward.

A Processor is used internally by a Study instance to execute pipelines
to derive derivatives requested from the data specification by the
\emph{data(name{[}, name,...{]})} method (Figure \ref{fig:simple_uml}). The first
step in this procedure is to query the repository tree for all data and
provenance associated with the study. Sessions for which the requested
outputs of the pipeline are already present in the repository, and the stored
provenance matches the current parameters of the study,
excluded from the list to process. For the remaining sessions to
process, inputs of the pipeline that are derivatives themselves are
added to the stack of requested derivatives. This procedure is repeated
recursively until there are no sessions to process or all inputs to the
pipeline are study inputs at a given depth.

When a pipeline is processed it is connected to source and
sink nodes, which get and put the pipeline inputs and outputs
from and to a repository, respectively. Separate source and sink nodes
are used for each data frequency (i.e. per-session, per-subject,
per-visit, per-study). If implicit file format conversion is required
(i.e. the input or output format differs from the data specification)
then additional format converter nodes are inserted after the source
nodes or before the sink nodes. Iterator nodes that iterate over the
required subjects and visits are connected to the sources, and ``deiterator''
nodes that join over subjects and visits are connected to the sink
nodes. Final nodes of upstream pipelines are connected to downstream
iterator nodes in order to create a single workflow, which is then executed using
the Nipype execution plugin.

Provenance is stored for each pipeline run alongside the generated
derivatives and consists of the supplied node parameter values,
software versions of external requirements, Arcana and Nipype used, 
a graph representation of the underlying Nipype workflow, checksums of inputs
and outputs, and any subject and visit IDs that were joined over in the workflow.

For subsequent analyses, changes with respect to any of the stored provenance values will
be flagged as a mismatch by default, with the exception of interface package versions
(e.g. Nipype or Banana versions). How mismatches are handled depends on the \emph{reprocess} flag passed to the
Processor. If \emph{reprocess} is true then the sessions with mismatching provenance
will be reprocessed, otherwise if \emph{reprocess} is false (the default) an exception will be raised.
Changes with respect to any element in the provenance JSON document can be ignored by providing
a list of JSONPath queries to the \emph{prov\_ignore} keyword argument of the Processor.

\subsubsection*{Environment modules}

The software packages installed on the system that are required by a
workflows (e.g. FSL, SPM) are detected and managed by
the Environment object passed to the study at initialisation.
There are currently two types of Environment class implemented in Arcana:
\emph{StaticEnv} and \emph{ModulesEnv}.

\emph{ModulesEnv} objects can be used if environment
modules \citep{furlani_modules:_1991} are installed on the system (typical on many
HPC systems). In this case, environment modules are loaded before a workflow node is run,
based on the requirements specified for the node during construction of the pipeline
(see \emph{Pipeline constructors}), and then unloaded afterwards. Mappings from non-standard
module names installed on the system to those expected by the Study can be
can be passed as a dictionary to the \emph{packages\_map} keyword argument
at initialisation. Likewise non-standard versions for packages can be mapped
onto the versioning system of a requirement using the \emph{versions\_map}
keyword argument.

\emph{StaticEnv} objects don't actively manage the environment,
and instead only check the current environment for appropriate software
versions before running requested workflows.

\subsection*{Acquisition of test dataset}

A healthy volunteer was scanned using a 3T Siemens Skyra with a 32-channel head
and neck coil to demonstrate the application of analyses implemented in
Arcana. The protocol was a T1-weighted MPRAGE (1mm contiguous,
matrix size 256x240x192, FOV 256x240x192, TE = 2.13ms, TR, = 2300ms, TI
= 900ms, bandwidth = 230Hz/pixel), GRE (1.8 mm contiguous, matrix size
256x232x72, FOV 230x208x130, TE = 20ms, TR, = 30ms,
bandwidth = 120Hz/pixel), and diffusion MRI (2 mm contiguous, matrix size
110x100x60, FOV 256x240x192, TE = 95ms, TR, = 8200ms, 33 diffusion
directions with b = 1500 mm2/s and 3 b=0, bandwidth = 781Hz/pixel).

\section*{Results}
\label{results}

The Arcana framework is distributed as a publicly available software
package via GitHub (\url{github.com/MonashBI/arcana}) and the
Python Package Index (PyPI) (\url{pypi.org/project/arcana/}).
Study classes for T1, T2* and diffusion-weighted MRI data have been
implemented as part of the \emph{Biomedical imging ANAlysis iN Arcana (Banana)} package
(\url{github.com/MonashBI/banana}; \url{pypi.org/project/banana}).
All three classes, \emph{T1Study}, \emph{T2starStudy} and
\emph{DwiStudy}, inherit generic image analysis methods, such as
registration and brain extraction, from the base class \emph{MriStudy}.

The DwiStudy class implements the extraction of diffusion tensor metrics,
FA and ADC, as well as whole-brain tractography using streamlines tracking from the MRtrix
toolbox \citep{tournier_improved_2010,tournier_mrtrix:_2012} (Figure \ref{fig:dwi_study}).

\begin{figure*}
	\centering
    \includegraphics[width=0.55\textwidth]{../figures/dwi_study}
  \caption{Example diffusion-weighted MRI (DWI) study, which can derive
tensor metrics, fractional anisotropy (FA) and apparent diffusion
coefficient (ADC) as well as streamlines fibre tracking. Blue boxes:
acquired (input) data (filesets or fields). Green boxes: derivatives.
Orange ovals: pipelines. Blue and green arrows: acquired and derived
inputs to pipelines, respectively. Orange arrows: outputs of pipelines.
The DWI magnitude image is preprocessed for motion correction and EPI
distortions masked and bias corrected. From the bias corrected image two
branches of analysis can be performed using the same intermediate
derivatives: FA and ADC and/or streamlines fibre tracking.}
\label{fig:dwi_study}
\end{figure*}

The \emph{T2starStudy} class implements an algorithm to generate
\emph{composite vein images} \citep{ward_combining_2018} and
vein masks \citep{ward_vein_2017} from the combination of
vein atlases derived from manual tracings with Quantitative
Susceptibility Mapping (QSM) and Susceptible Weighted Imaging (SWI)
images derived from the T2*-weighted acquisition (Figure \ref{fig:t2star_t1_study}).

\begin{figure*}
	\centering
    \includegraphics[width=0.775\textwidth]{../figures/t2star_t1_study}
  \caption{Combined T2*/T1-weighted studies within \emph{ArcanaPaper}
  MultiStudy class, which can derive vein masks by combining Quantitative Susceptibility
Mapping (QSM) and Susceptible Weighted Imaging (SWI) contrasts with a
manual atlas. Blue boxes: input data (filesets or fields).
Green boxes: derivatives. Orange ovals: pipelines. Blue and green
arrows: inputs to pipelines from input and derived data, respectively. Orange
arrows: outputs of pipelines. Dashed boxes represent data specifications
in a sub-study that are present in the global namespace and mapped into
the sub-study space, and dotted arrows the mappings. The acquired
T1-weighted image is mapped to both the \emph{magnitude} spec of the
T1-weighted sub-study and the \emph{registration reference} spec of the
T2*-weighted sub-study. The nonlinear transformation from subject to
atlas space are mapped from the T1-weighted sub-study and combined with
the linear registration between T1-weighted and T2*-weighted images, QSM
and SWI images are combined to produce the composite-vein image. In this
instance, the SWI acquired from the scanner console is passed
as an input to the derived \emph{SWI} specification, overriding the
\emph{SWI} pipeline that would otherwise generate it (dashed oval).}
\label{fig:t2star_t1_study}
\end{figure*} 

The T1Study, T2starStudy and DwiStudy classes are
aggregated into a single MultiStudy class,
\emph{ArcanaPaper} (Supplementary material), which is specialised to produce
the figures in the Results section of this manuscript. In order to warp the
vein atlases to the subject space for comparison with the SWI and QSM images,
nonlinear registration to Montreal Neurological Institute templates
\citep{grabner_symmetric_2006} is performed in the T1Study. The transform and warp
field from this registration are then mapped onto the `coreg\_to\_atlas\_mat` and
`coreg\_to\_atlas\_warp' specifications in the T2starStudy, as the registration of
T2*-weighted images to the MNI template is typically poor.
These transforms are combined with the linear transform from the
brain-extracted T2*-weighted magnitude image to the brain extracted T1-weighted image,
which requires the `brain' specification in the T1Study to be mapped
to the `coreg\_ref\_brain` specification in the T2starStudy.

The SWI image reconstructed on the scanner console is substituted for the \emph{SWI} derivative
produced by the \emph{SWI} pipeline of the T2starStudy. For ease of comparison
with the QSM and vein images produced by T2starStudy, this SWI image is brain extracted
in a separate MriStudy sub-study of ArcanaPaper.

The \emph{vein\_fig}, \emph{fa\_adc\_fig} and \emph{tractography\_fig} methods
implemented in the ArcanaPaper class were applied to the test dataset to produce
Figure \ref{fig:veins}, \ref{fig:fa_adc}, and \ref{fig:tractography}, respectively.
Figure \ref{fig:veins} displays composite vein images and vein masks for the healthy
volunteer along with the SWI and QSM intermediate derivatives.
The derived vein images are comparable to those
generated by the original implementation \citep{ward_combining_2018}.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{../figures/veins}
  \caption{Composite vein image (\emph{third row}) constructed by
  combining susceptibility weighted imaging (SWI) (\emph{top row}),
  quantitative susceptibility mapping (QSM) (\emph{second row})
  and a vein atlas from manual tracings. A vein mask was then
  generated (\emph{bottom row}) from the composite
  vein image.
  \emph{left column}: axial slices. \emph{centre column}: coronal slices.
  \emph{right column}: sagittal slices.}
\label{fig:veins}
\end{figure*}

Figure \ref{fig:fa_adc}  displays the FA and ADC maps derived from the diffusion MRI
acquisition. The FA map shows high intensity in known white matter
tracts and low intensities in known grey matter regions. The ADC map
shows high intensity in cortical spinal fluid and low intensity through
the rest of the brain.
Figure \ref{fig:tractography} displays the global tractography derived from the DWI
acquisition. The streamlines follow well known white matter tracts such
as the cortico-spinal, fasciculus and corpus callosum. Intermediate
derivatives derived for the FA and ADC analysis, including the
preprocessed and bias-corrected DWI image and a whole brain mask, were
reused in the generation of the streamlines.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{../figures/fa_adc}
  \caption{Fractional Anisotropy (FA) \emph{(top row)} and
Apparent Diffusion Coefficient (ADC) (\emph{bottom rowI)} derived from
diffusion MRI data. \emph{Left column:} axial midline slices.
\emph{Middle column:} coronal midline slices. \emph{Left column}:
sagittal midline slices.}
\label{fig:fa_adc}
\end{figure*}


\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{../figures/tractography}
  \caption{Global tractography performed using the MRtrix
toolbox. Probabilistic streamlines generated with the iFOD2 algorithm
from fibre Orientation Distribution Function (fODF) estimated from
diffusion MRI datasets using Constrained Spherical Deconvolution (CSD).
Streamlines are colour-encoded by orientation: green=anterior-posterior,
blue=inferior-superior, red=left-right. \emph{Left panel:} axial midline
slice. \emph{Middle panel:} coronal midline slice. \emph{Left panel}:
sagittal midline slice.}
\label{fig:tractography}
\end{figure*}

\section*{Discussion}
\label{discussion}

We present Arcana, a software framework to facilitate the development of
analysis suites for neuroimaging data that implement
complete workflows from repository data to publication results. The
encapsulation of repository data and workflow generation in Arcana
enables researchers to create robust workflows while focussing on the
core logic of their analysis. Arcana's modular pipeline and OO
architecture promotes code reuse between different workflows by
facilitating the sharing of common segments (e.g. registration,
segmentation). The clear separation of analysis design from its
application leads to portable workflows, which can be applied to
datasets stored in a number of storage systems. In addition, the
management of intermediate derivatives, provenance and software
versioning, coupled with ability to submit jobs to HPC clusters, enables
workflows implemented in Arcana to scale to large datasets. Researchers
are able to quickly prototype analyses that can be deployed to large-scale
infrastructure on local workstations without modification.

Software frameworks \citep{yacoub_pattern-oriented_2004} have been successful in
improving code quality and efficiency of development in a variety of
contexts \citep{moore_professional_2008,white_hadoop:_2012,abadi_tensorflow:_2016}. By
factoring out common elements, only features that are specific to the
given application need to be implemented by the analysis designer, and
the common elements become battle hardened through repeated use. Arcana
handles many of the menial tasks involved with workflow implementation,
such as data retrieval and storage, format conversions, and provenance,
reducing the time and effort required to implement robust workflows.

An oft-repeated mantra in the open-source software movement dubbed
Linus' Law is that ``given a large enough beta-tester and co-developer
base, almost every problem will be characterized quickly and the fix
obvious to someone" or more compactly, ``given enough eyeballs, all
\href{https://en.wikipedia.org/wiki/Software_bug}{bugs} are shallow"
\citep{raymond_cathedral_1999}. Given the size of the neuroimaging research community,
there are a large number of potential beta-testers and co-developers.
However, it has been difficult for researchers to collaborate on the
same code-base due to slight differences in acquisition protocols,
storage conventions, researcher preferences, and study requirements.

The flexibility and portability of the Arcana framework increases the
feasibility of community collaborations on workflow implementations. The
improvement of code quality in larger community efforts, due to more
eyeballs to detect and fix errors, has the potential to form a
reinforcing cycle where more developers are attracted to the project.
To these ends, the Banana code repository on GitHub
(\url{github.com/MonashBI/banana.git}) is proposed as a code base
for communal development of biomedical imaging workflows using Arcana.

A level of proficiency in Python OO design is required to implement new
analyses in Arcana, which may preclude inexperienced programmers.
However, only a basic knowledge of Python is required to apply existing
analyses to new datasets. Furthermore, a number of example Study classes
have been implemented, which can guide the hand of analysis designers.
Arcana imposes a consistent structure on workflows implemented within
it, making the code easier to understand for developers who are familiar
with the framework. In addition, class inheritance provides a manageable
way to adapt and extend existing analyses, and highlights where
modified analyses differ from standard procedures.

MR contrast-specific analyses are implemented in Banana via a chain
of successively specialised Study sub-classes (e.g.
MRI\textgreater{}EPI\textgreater{}DWI) to enable generic processing
steps (e.g. registration) to be shared between classes. While not
necessary, it is recommended to create a subclass specific to the
research study in question and aggregate all related analysis within it.
Since such classes can be applied to alternate datasets in order to
reproduce the exact analysis. The \emph{ArcanaPaper} class
(Supplementary material), which contains methods to generate all figures
in the Results section of this manuscript, is an example of this approach.

The abstraction of data and repositories in Arcana enables the same
workflow implementation to be applied to datasets stored in BIDS format
or XNAT repositories. A single code-base can therefore be containerized
into BIDS apps or XNAT pipelines without adaptation, helping to form a
bridge between the two communities of users and developers. Alternative
data storage systems \citep{scott_coins:_2011,das_loris:_2012,book_neuroinformatics_2013},
can be integrated into Arcana by overriding a small number of
methods from the Repository abstract base class. Repository modules
could also be created for data portals such as \emph{DataLad}
\citep{yaroslav_halchenko_datalad/datalad_2018}
in order to take advantage of the range of platforms they support.
Implementing analyses in Arcana therefore enables researchers and
research groups to easily migrate their workflows between storage
platforms, and not risk being locked in to a particular technology.

While Arcana was primarily developed for neuroimaging datasets, it is a
general framework that could be applied to data from other fields.
However, in other contexts, the subject and visit hierarchy may no
longer make sense. In many cases it may be sufficient to map subjects
and/or visits onto alternative concepts (e.g. for meteorological data
\emph{subjects} = \emph{weather stations}, \emph{visits = observation
times}). But some cases may require a deeper data hierarchy (i.e.
greater than two), which is not currently possible in Arcana.

Ensuring that the consistent versions of external tools are used
throughout the analysis is important to avoid introducing biases due to
algorithm updates. In systems with environment modules \citep{furlani_modules:_1991}
installed, Arcana can load and unload the required modules before and
after each node is executed. When running Arcana within a container,
environment modules and software versions can be installed inside the
container giving exact control over the versions used. To these ends, a
Docker container is available on Docker Hub,
(\url{hub.docker.com/r/monashbi/banana}), which can be used as a
base for biomedical imaging analysis containers. In future versions of Arcana,
additional Environment modules could be implemented to run each pipeline
node within its own container to take advantage of
containers maintained by tool developers (e.g. 
\url{hub.docker.com/r/vistalab/freesurfer/}).

While the same tools and versions should be applied across an analysis
to avoid bias, there are cases where it is desirable to rerun the same
analysis with different tools substituted at various points in the
workflow. In particular, when introducing new tools or upgrades to
existing tools, it is important to show the effect on the final results
in comparison with existing methods. Furthermore, it is typically not
clear what variability between results produced by comparable tools is
due to. Therefore, in the absence of \emph{a priori} reason to favour a
particular tool, perhaps the most rigorous approach is to rerun analyses
with different combinations of available tools and only present results
that are robust to the ``analytic noise'' \citep{maumet:inserm-01886089} they introduce.
Switch parameters make it straightforward to rerun analyses in Arcana
with substituted tools while controlling all other aspects of the
workflow.

The management of intermediate derivatives and provenance in Arcana
guarantees that the same analysis is applied across the dataset without
necessarily requiring a complete rerun of the analysis. This guarantee
makes it feasible to process data as it is acquired over the course of
long studies, and therefore help detect any problems that might arise
with the acquisition protocol when they occur. In addition, by reusing
shared intermediate derivatives between analyses, such as the
preprocessed DWI shared between tensor and fibre tracking workflows
(Figure \ref{fig:fa_adc} and \ref{fig:tractography}), processing time as
well as time required for manual QC is minimised. Given analyses implemented
in Arcana are also able to be processed on HPC clusters, they scale well to large studies.

The framework provided by Arcana should aid the development of workflows
that are consistent with the F.A.I.R. guiding principles \citep{wilkinson_fair_2016}.
Each pipeline is accompanied by a text description and
a list of citations to describe the techniques it employs.
The extensibility and modularity of workflows in Arcana should encourage
their reuse, and enable communal development of code repositories.
Arcana analyses are interoperable between different
repository and execution environments. In addition, the development of
Banana is planned to closely follow the specification for BIDS
derivatives \citep{gorgolewski_bids_nodate} for interoperability with
other BIDS apps.

\section*{Conclusion}
\label{conclusion}

By managing the complete flow of data from/to a repository with modular
components, Arcana enables complex analyses of large-scale neuroimaging
studies that are portable across a wide range of research sites. The
extensibility of analyses implemented in Arcana, coupled with the
flexibility afforded by programmatic constructuction of pipelines,
facilitates the design of comprehensive analyses by larger communities.
Larger communities of developers working on the same code-base should
make it feasible to capture the arcana of neuroimaging analysis in
templates that can be applied to a wide range of relevant datasets.

\begin{acknowledgements}
The authors acknowledge the facilities and scientific and technical assistance
of the National Imaging Facility, a National Collaborative Research
Infrastructure Strategy (NCRIS) capability, at Monash Biomedical Imaging,
Monash University. The ``transparent repository'' feature of Arcana was
inspired by in-house software written by Parnesh Raniga while he was
employed at Monash University prior to 2016.
\end{acknowledgements}

\bibliographystyle{spbasic} 
\bibliography{arcana}

\end{document}
